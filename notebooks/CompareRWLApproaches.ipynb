{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(repr)\n",
    "options(repr.plot.width=11, repr.plot.height=7, repr.plot.res = 80, repr.plot.quality = 40)\n",
    "options(jupyter.plot_mimetypes = 'image/png')\n",
    "\n",
    "setwd(\"~/yandexDisk/DIPLOMA/CODE/src\")\n",
    "source(\"simulations.functions.R\", echo=FALSE)\n",
    "source(\"../../OWL/O_learning_functions.r\", echo=FALSE)\n",
    "debug.file <- \".various.Rdata/Iter.info\"\n",
    "set.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining control execution constants \n",
    "train.data.sample.sizes <- c(50, 100, 200, 400, 800)\n",
    "test.data.sample.size <- 10000\n",
    "sample.size <- 800\n",
    "offsets = seq(0.2, 1, length = 10)\n",
    "control.offset = min(offsets) / 2\n",
    "lambdas = seq(0,  10,  length=12)\n",
    "scenario = \"chen.2\"\n",
    "\n",
    "registerDoParallel(cores = 4)\n",
    "number.of.iters <- 200\n",
    "\n",
    "offset = 0.1\n",
    "lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test <- GetSimulationData(test.data.sample.size, scenario = scenario)\n",
    "train <- GetSimulationData(sample.size, scenario = scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation specific functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличаются только тем, что GetInitPars не вычитает min(quantile, 0), а сразу берет 1-q наилучших наград"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_model <- with(train, lm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))\n",
    "# train$reward <- residuals(lm_model)\n",
    "test.prediction <- with(test, predict(lm_model, data.frame(raw.R = raw.reward, covariates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(test$raw.reward, test.prediction, col=rgb(0, 0.4, 0.5, 0.05), pch=19)\n",
    "abline(a=0, b=1, col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rlm_model <- with(train, rlm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (n in names(rewards.q.adjusted)) {\n",
    "    print(n)\n",
    "    reward.adj = rewards.q.adjusted[[n]]\n",
    "    train$reward <- reward.adj\n",
    "    OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model <- with(train, randomForest(covariates,  raw.reward, ntree=1000, mtry=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with(test, hist(predict(rrf_model, covariates), breaks=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with(train, qplot(raw.reward, raw.reward - predict(rrf_model, covariates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_model <- with(train, lm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))\n",
    "with(train, qplot(raw.reward, residuals(lm_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l <- with(train, lowess(data.frame(raw.R = raw.reward, covariates[, -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm_model <- with(train, train(raw.R ~ ., data=data.frame(raw.R = raw.reward, covariates[, -1]), method=\"gbm\", tuneLength = 7))\n",
    "plot(gbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rrf_model <- with(train, train(raw.R ~ ., data=data.frame(raw.R = raw.reward, covariates[, -1]), method=\"rf\", tuneLength = 7 ))\n",
    "plot(rrf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quants <- seq(10, 90, 10) / 100\n",
    "\n",
    "for (scenario in c(\"chen.1\", \"chen.2\")) {\n",
    "    global.stat <- numeric()\n",
    "    global.abs.dev <- numeric()\n",
    "    test <- GetSimulationData(test.data.sample.size, scenario = scenario) \n",
    "    for (i in seq(1,number.of.iters)) { \n",
    "        train <- GetSimulationData(sample.size, scenario = scenario)\n",
    "        ## Without correction\n",
    "        init.pars = GetOwlParams(train, lambda, weights = F)\n",
    "        opt.params <- list(\"opt.func\"=DCOptimizeWithMML2PenalizedProperIters, \"tolerance\"=1e-7, \"init.pars\"=init.pars, \"approximation.eps\"=1e-7)\n",
    "        pars.dca.MM <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "        pars.owl.dc_loop <- GetDCLoopPars(train, offset, lambda, init.pars)\n",
    "        ## Correction\n",
    "        lm_model <- with(train, lm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))\n",
    "        train$reward <- residuals(lm_model)\n",
    "        pars.dca.MM.res <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "        train$reward <- with(train, raw.reward - mean(raw.reward))\n",
    "        pars.dca.MM.mean.extracted <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "        rewards.q.adjusted <- lapply(quantile(train$raw.reward, quants), function(q) with(train, {raw.reward - q}))\n",
    "        names(rewards.q.adjusted) <- sapply(names(rewards.q.adjusted), function(n) paste(\"MM\", n, \"adjusted\", sep = \".\"))\n",
    "        pars.dca.MM.quants.extracted <- lapply(rewards.q.adjusted, function(reward.adj) {\n",
    "            train$reward <- reward.adj\n",
    "            OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "        })\n",
    "        \n",
    "        # experiment with randomForest \n",
    "        rf_model <- with(train, randomForest(covariates,  raw.reward, ntree=1000, mtry=3)) \n",
    "        train$reward <- with(train, raw.reward - predict(rf_model, covariates))\n",
    "        pars.dca.MM.res.rrf <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "            \n",
    "        # experiment with boosting\n",
    "        gbm_model <- with(train, train(raw.R ~ ., data=data.frame(raw.R = raw.reward, covariates[, -1]), method=\"gbm\", tuneLength = 7, verbose = FALSE))\n",
    "        train$reward <- with(train, raw.reward - predict(gbm_model, covariates))\n",
    "        pars.dca.MM.res.gbm <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "        \n",
    "            \n",
    "            \n",
    "        data.list <- list(\"train\"=train, \"test\"=test)\n",
    "        params.list  <- list(\n",
    "                             \"OWL.Init.pars\"=init.pars,\n",
    "                             \"MM\"=pars.dca.MM, \n",
    "                             \"OWL.dc_loop\"=pars.owl.dc_loop, \n",
    "                             \"MM.lm.resi\"=pars.dca.MM.res, \n",
    "                             \"MM.Mean.extracted\" = pars.dca.MM.mean.extracted, \n",
    "                             \"MM.rrf.resi\" = pars.dca.MM.res.rrf, \n",
    "                             \"MM.gbm.resi\" = pars.dca.MM.res.gbm\n",
    "                            ) \n",
    "        params.list <- c(params.list, pars.dca.MM.quants.extracted)\n",
    "        iter.stat <- GetMetricsForParams(params.list, data.list, offset, PolicyFunLinearKernel, lambda)\n",
    "        global.stat  <- rbind(global.stat, iter.stat)\n",
    "        mean.abs.deviation <- sapply(params.list, \n",
    "            function(p) with(test, mean(abs(optimal.treatment - PolicyFunLinearKernel(p, covariates)))))\n",
    "        global.abs.dev <- rbind(global.abs.dev, mean.abs.deviation)\n",
    "\n",
    "    }\n",
    "\n",
    "    rownames(global.stat) <- NULL\n",
    "    q.test <- melt(global.stat[, grep(\"Q.*TEST\", colnames(global.stat))])\n",
    "    gg <- ggplot(q.test)  + \n",
    "        geom_boxplot(aes(X2, value, color=X2)) + \n",
    "        theme(axis.text.x=element_text(angle = 70, hjust=1), text = element_text(size=15)) +         \n",
    "        ggtitle(paste(\"True Q-function value for predicted treatment:\", scenario))\n",
    "    gg <- gg + coord_cartesian(ylim=quantile(gg$data$value, c(0.1, 0.9))) \n",
    "    print(gg)\n",
    "    rownames(global.abs.dev) <- NULL\n",
    "    gg <- ggplot(melt(global.abs.dev)) + \n",
    "        geom_boxplot(aes(X2, value,  colour=X2)) + \n",
    "        theme(axis.text.x=element_text(angle = 70, hjust = 1), text = element_text(size=15)) + \n",
    "        ggtitle(paste(\"Mean absolute deviation from ideal treatment:\", scenario))\n",
    "    gg <- gg + coord_cartesian(ylim=quantile(gg$data$value, c(0.1, 0.9))) \n",
    "    print(gg)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with substracting different statistics from Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Substract regression of R on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_model <- with(train, lm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))\n",
    "train$reward <- residuals(lm_model)\n",
    "test.prediction <- with(test, predict(lm_model, data.frame(raw.R = raw.reward, covariates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train$reward <- residuals(lm_model)\n",
    "test$reward <- with(test, raw.reward - predict(lm_model, data.frame(raw.R = raw.reward, covariates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with substracting different statistics from Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Substract regression of R on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GetDCLoopPars(train, offset, lambda, init.pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global.stat <- numeric()\n",
    "global.abs.dev <- numeric()\n",
    "\n",
    "for (i in seq(1,number.of.iters)) {\n",
    "    train <- GetSimulationData(sample.size, number.of.covariates, scenario = scenario)\n",
    "    pars.owl.dc_loop <- GetDCLoopPars(train, offset, lambda, init.pars)\n",
    "    lm_model <- with(train, lm(raw.R ~ . - 1, data=data.frame(raw.R = raw.reward, covariates)))\n",
    "    train$reward <- residuals(lm_model)\n",
    "    init.pars = GetOwlParams(train, lambda, weights = F)\n",
    "    opt.params <- list(\"opt.func\"=DCOptimizeWithMML2PenalizedProperIters, \"tolerance\"=1e-7, \"init.pars\"=init.pars, \"approximation.eps\"=1e-7)\n",
    "    pars.dca.MM <- OptimizeParamsOfPolicyFunction(train, offset, PolicyFunLinearKernel, lambda, opt=opt.params)\n",
    "\n",
    "    data.list <- list(\"train\"=train, \"test\"=test)\n",
    "    params.list  <- list(\n",
    "                         \"OWL.Init.pars\"=init.pars,\n",
    "                         \"MM.approx\"=pars.dca.MM, \n",
    "                         \"OWL.dc_loop\"=pars.owl.dc_loop\n",
    "                        ) \n",
    "    iter.stat <- GetMetricsForParams(params.list, data.list, offset, PolicyFunLinearKernel, lambda)\n",
    "    global.stat  <- rbind(global.stat, iter.stat)\n",
    "    mean.abs.deviation <- sapply(params.list, \n",
    "        function(p) with(test, mean(abs(optimal.treatment - PolicyFunLinearKernel(p, covariates)))))\n",
    "    global.abs.dev <- rbind(global.abs.dev, mean.abs.deviation)\n",
    "    \n",
    "}\n",
    "                                \n",
    "rownames(global.stat) <- NULL\n",
    "q.test <- melt(global.stat[, grep(\"Q.*TEST\", colnames(global.stat))])\n",
    "ggplot(q.test, aes(X1, value))  + geom_boxplot(aes(colour = X2)) + theme(text = element_text(size=15))\n",
    "                                 \n",
    "rownames(global.abs.dev) <- NULL\n",
    "ggplot(melt(global.abs.dev)) + geom_boxplot(aes(X2, value,  colour=X2)) + theme(text = element_text(size=15))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile regression is better then dc_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GetDCLoopPars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GetDCLoopPars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (scenario in c(\"chen.1\")) {\n",
    "    global.stat <- numeric()\n",
    "    global.abs.dev <- numeric()\n",
    "    test <- GetSimulationData(test.data.sample.size, scenario = scenario) \n",
    "    for (i in seq(1, number.of.iters)) { \n",
    "        train <- GetSimulationData(sample.size, scenario = scenario)\n",
    "        ## Without correction\n",
    "        init.pars = GetOwlParams(train, lambda, weights = F)\n",
    "        opt.params <- list(\"opt.func\"=DCOptimizeWithMML2PenalizedProperIters, \"tolerance\"=1e-7, \"init.pars\"=init.pars, \"approximation.eps\"=1e-7)\n",
    "        pars.owl.dc_loop <- GetDCLoopPars(train, offset, lambda, init.pars)\n",
    "\n",
    "\n",
    "        data.list <- list(\"train\"=train, \"test\"=test)\n",
    "        params.list  <- list(\n",
    "                             \"OWL.Init.pars\"=init.pars,\n",
    "                             \"OWL.dc_loop\"=pars.owl.dc_loop\n",
    "                        ) \n",
    "        iter.stat <- GetMetricsForParams(params.list, data.list, offset, PolicyFunLinearKernel, lambda)\n",
    "        global.stat  <- rbind(global.stat, iter.stat)\n",
    "        mean.abs.deviation <- sapply(params.list, \n",
    "            function(p) with(test, mean(abs(optimal.treatment - PolicyFunLinearKernel(p, covariates)))))\n",
    "        global.abs.dev <- rbind(global.abs.dev, mean.abs.deviation)\n",
    "    }\n",
    "\n",
    "    rownames(global.stat) <- NULL\n",
    "    q.test <- melt(global.stat[, grep(\"Q.*TEST\", colnames(global.stat))])\n",
    "    gg <- ggplot(q.test)  + \n",
    "        geom_boxplot(aes(X2, value, color=X2)) + \n",
    "        theme(text = element_text(size=15)) +         \n",
    "#         theme(axis.text.x=element_text(angle = 0, hjust=0), text = element_text(size=15)) +         \n",
    "        ggtitle(\"Scenario 1 - True Q-function value based on 200 train procedures\")\n",
    "#     gg <- gg + coord_cartesian(ylim=quantile(gg$data$value, c(0.1, 0.9))) \n",
    "    print(gg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique(q.test$X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edit(dc_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "named.labels <- c( \"Qfun.OWL.dc_loop.TEST\" = \" dc_loop function\", \"Qfun.OWL.Init.pars.TEST\" = \" single quantile regression\")\n",
    "labels <- c( \"Qfun.OWL.dc_loop.TEST\" = \" dc_loop function\", \"Qfun.OWL.Init.pars.TEST\" = \" single quantile regression\")\n",
    "gg <- ggplot(q.test)  + \n",
    "    geom_boxplot(aes(X2, value, color=X2)) + \n",
    "    theme(text = element_text(size=15)) +         \n",
    "    labs(title = \"Scenario 1 - Expected Value-function (200 train procedures)\", \n",
    "         x = \"\", y = \"Value-function\", color = \"Algorithm\") +\n",
    "    scale_x_discrete(labels=labels) +\n",
    "    scale_colour_manual(labels = labels , values= c(\"dark green\", \"red\"))\n",
    "print(gg)\n",
    "ggsave(filename=\"1.png\", plot=gg, width = 9, height = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?waiver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?scale_x_discrete\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
